{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d215aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/opt/anaconda3/envs/simsim/lib/python3.9/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "from config import config\n",
    "from config import update_config\n",
    "from models import build_model\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47226916",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(cfg='experiments/imagenet/swin/swin_tiny_patch4_window7_224.yaml', opts=[], rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773fad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from experiments/imagenet/swin/swin_tiny_patch4_window7_224.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/opt/anaconda3/envs/simsim/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484611838/work/aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "update_config(config, args)\n",
    "model = build_model(config, is_teacher=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e46a1d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=96, input_resolution=(56, 56), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(7, 7), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3 mlp_ratio=4\n",
       "          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(7, 7), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(56, 56), dim=96\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=192, input_resolution=(28, 28), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(7, 7), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3 mlp_ratio=4\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(7, 7), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(28, 28), dim=192\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=384, input_resolution=(14, 14), depth=6\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3 mlp_ratio=4\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3 mlp_ratio=4\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3 mlp_ratio=4\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(14, 14), dim=384\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=768, input_resolution=(7, 7), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0 mlp_ratio=4\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c540cbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.3164e-01, -5.2792e-01, -8.5244e-02,  1.0834e+00,  4.8584e-01,\n",
       "          1.3233e-01,  4.0017e-01, -1.4926e+00, -9.7831e-02,  4.9397e-01,\n",
       "         -4.9904e-01, -1.0827e-01, -7.1403e-02,  5.2481e-01, -2.4763e-01,\n",
       "          1.3289e-01,  1.3957e-01, -3.5992e-01,  1.4561e-01,  1.2023e+00,\n",
       "         -4.9652e-01, -4.4444e-01,  6.2408e-01, -6.5475e-01, -4.4438e-01,\n",
       "          2.9058e-01, -1.3107e-01,  8.6393e-01,  6.6566e-01,  1.1424e-01,\n",
       "          1.5558e-02, -4.2270e-03, -3.9559e-01, -1.1066e+00, -1.0287e-02,\n",
       "         -1.2965e+00, -3.1601e-01, -1.0262e+00, -6.1733e-01,  5.6499e-01,\n",
       "         -5.9375e-01,  3.9921e-01, -1.4473e-01,  5.7900e-02,  1.2649e-01,\n",
       "         -2.3177e-01, -5.7732e-01, -2.8365e-01, -5.3333e-01, -3.4946e-01,\n",
       "         -4.6756e-01,  9.3533e-02, -1.2503e-01,  9.6771e-03, -3.2504e-01,\n",
       "          2.7054e-02, -5.5648e-02,  4.9590e-01, -5.2966e-02,  9.7972e-01,\n",
       "         -5.0449e-01, -6.2460e-01,  6.0479e-01,  1.6492e-01, -1.9412e-01,\n",
       "         -5.6811e-01,  1.4584e+00,  7.4004e-01,  3.6432e-01, -3.0510e-02,\n",
       "         -3.5396e-01,  1.9299e-01,  1.1982e-01,  9.8222e-01, -4.0505e-01,\n",
       "          2.8571e-01,  3.0339e-01, -5.3735e-01,  3.3129e-03, -1.1095e-01,\n",
       "         -2.0824e-01,  2.5563e-01,  5.5163e-01, -2.2017e-01, -1.7321e-01,\n",
       "          1.7492e-01, -6.7024e-01, -3.2611e-01,  6.8683e-01,  3.3926e-02,\n",
       "         -1.0308e-01, -7.1279e-01, -5.0129e-01, -1.3835e-01, -3.9305e-01,\n",
       "         -1.5237e-01, -3.6776e-01,  3.1823e-01,  1.8274e-01,  7.8537e-01,\n",
       "         -5.7894e-01, -6.8820e-01,  5.4824e-01,  2.6721e-01, -6.8043e-02,\n",
       "          6.4724e-01,  8.5372e-01,  5.7197e-01, -1.5217e-01, -1.0397e+00,\n",
       "          4.2234e-01,  1.2407e-01, -4.3171e-01,  2.9342e-01,  2.3431e-01,\n",
       "          5.7482e-01,  1.2811e-01, -6.4831e-01, -5.5103e-01, -2.3011e-01,\n",
       "         -3.1708e-01, -1.3560e-01, -1.4694e-01, -6.5451e-01,  7.2479e-02,\n",
       "         -1.9916e-01, -1.8692e-01,  2.8337e-01,  7.8012e-01,  3.6234e-01,\n",
       "          6.9401e-01,  3.0179e-01, -4.0323e-01,  2.1247e-01,  5.5080e-01,\n",
       "         -4.9686e-01, -1.3698e-01,  5.1151e-01,  1.3125e-02,  2.4823e-01,\n",
       "          8.2204e-01, -6.5347e-01,  4.0878e-01,  6.9262e-01, -2.0843e-01,\n",
       "          4.6212e-01,  7.5979e-01, -3.7354e-01, -6.9144e-01, -2.9341e-01,\n",
       "          2.5173e-01,  1.1715e-01, -2.4673e-01, -4.7132e-01,  2.0201e-01,\n",
       "          4.3122e-01, -6.6340e-03, -4.0466e-01,  4.6132e-01, -1.5008e-01,\n",
       "          3.3358e-01,  2.2235e-01, -2.7642e-01, -3.6037e-01, -6.4460e-01,\n",
       "          4.0236e-01,  6.9523e-03, -6.3193e-01,  1.3273e-02,  4.2483e-01,\n",
       "          2.9860e-01, -6.7045e-01, -3.9534e-01,  1.9437e-01,  5.1040e-01,\n",
       "          5.3759e-01, -8.4214e-01,  1.2439e-01,  2.6671e-01, -5.1580e-01,\n",
       "          4.5904e-02,  6.2865e-01,  1.5211e-01, -9.8639e-02, -7.1701e-01,\n",
       "          7.8009e-02, -2.6426e-01, -1.1949e-01,  6.2242e-01, -9.3285e-02,\n",
       "         -2.8438e-01,  1.4106e-01,  5.5819e-01, -6.4372e-01,  9.0930e-02,\n",
       "          1.6731e-01, -6.2504e-01, -5.9898e-01,  5.1955e-01,  6.6811e-01,\n",
       "         -2.0029e-01,  3.9512e-01,  1.4653e-01,  5.4600e-01, -5.0395e-01,\n",
       "         -4.0463e-01,  7.8801e-01, -8.9361e-01, -3.9409e-01, -4.1829e-02,\n",
       "          1.1418e+00,  7.5086e-01, -1.8733e-01, -8.7716e-01, -8.6443e-02,\n",
       "         -2.2841e-01, -9.5627e-01, -4.6374e-02,  6.6845e-01,  1.0063e+00,\n",
       "         -7.1489e-02,  1.3285e-01,  8.4934e-01, -5.8136e-01,  5.0652e-01,\n",
       "          1.6518e-01,  9.5217e-01, -9.2802e-01, -1.8023e-01,  7.9541e-02,\n",
       "         -4.9476e-01, -1.8808e-01,  3.8248e-02, -7.3460e-02,  3.5982e-01,\n",
       "          1.4093e-01, -6.5002e-01, -6.5829e-01, -4.1938e-01,  5.3835e-01,\n",
       "         -2.0892e-01, -2.5358e-01,  1.3917e-01,  1.2842e+00, -5.5463e-01,\n",
       "          4.1534e-01, -8.9994e-02, -6.1960e-02,  3.7480e-01, -3.3146e-02,\n",
       "         -1.1099e+00,  6.7989e-01,  2.6728e-02,  2.8564e-01, -3.5449e-01,\n",
       "         -1.9503e-01, -2.2431e-01, -1.3270e+00,  6.7333e-01,  4.0817e-01,\n",
       "         -1.1393e-01,  9.3009e-02,  5.0290e-01, -2.1025e-01, -2.0152e-01,\n",
       "         -9.2417e-01,  9.5880e-02,  4.2239e-01, -4.6985e-02, -6.4781e-01,\n",
       "         -3.1558e-01, -4.0956e-01, -5.1631e-02, -1.9224e-01,  1.7244e-01,\n",
       "         -9.8457e-01, -3.0532e-01, -1.7422e-01, -6.4545e-01,  8.5947e-03,\n",
       "          3.4796e-01,  4.8573e-01, -1.9550e-01, -1.7068e-01, -7.7857e-01,\n",
       "          5.2911e-01,  6.9492e-01,  2.1261e-01,  9.0871e-01,  3.8114e-01,\n",
       "          1.2146e-02, -6.4934e-01, -1.3054e-01, -8.4765e-01,  6.6296e-01,\n",
       "          3.9309e-01,  2.9624e-01,  2.9364e-02, -2.8645e-01, -2.1860e-01,\n",
       "         -3.2650e-01,  6.0872e-01,  4.2496e-01, -3.4180e-01, -5.7582e-01,\n",
       "          1.0341e+00,  8.4530e-01, -1.5004e-02,  8.5674e-01,  1.1545e-01,\n",
       "          7.4472e-01,  2.3954e-01, -1.1919e-01, -6.8024e-01,  4.0944e-01,\n",
       "          5.6545e-01,  8.6174e-02,  2.4577e-01, -9.6560e-02,  5.3452e-01,\n",
       "          1.9422e-02, -4.2935e-01, -1.2131e+00, -2.2600e-01, -2.4001e-01,\n",
       "         -1.4450e-01,  1.1407e-01, -9.3908e-02, -8.3535e-02,  4.7360e-01,\n",
       "          6.4414e-01,  2.2567e-01, -7.1638e-02, -9.1029e-02, -4.9340e-01,\n",
       "         -6.6774e-03,  2.3750e-01, -3.3991e-02,  1.9804e-01,  6.2417e-01,\n",
       "         -2.0411e-01, -9.6468e-02,  6.5613e-01, -1.0261e+00,  7.5755e-01,\n",
       "          7.7795e-01,  1.0364e+00, -4.1840e-02,  3.7924e-01, -5.7796e-01,\n",
       "         -5.2038e-01,  6.6200e-01,  8.1358e-01, -1.6466e-01,  1.2064e+00,\n",
       "         -6.0433e-01,  3.2315e-01, -3.0501e-01, -5.7591e-01, -9.4789e-01,\n",
       "         -3.2392e-01,  4.5251e-01,  2.2271e-01, -4.4786e-01, -1.1324e+00,\n",
       "         -1.8005e-01,  1.1046e-01,  2.9272e-02, -4.4825e-01,  7.0483e-01,\n",
       "         -7.3856e-01, -5.2918e-02,  4.2230e-01,  3.5620e-01,  5.1679e-02,\n",
       "         -1.9273e-01,  7.0934e-01, -5.2621e-01,  2.4222e-01, -8.3165e-01,\n",
       "         -1.9874e-01,  1.1537e+00,  3.0221e-01, -5.3903e-01,  3.6125e-01,\n",
       "          1.1646e-01,  7.2573e-01, -3.1690e-01, -4.9624e-01, -3.7815e-01,\n",
       "         -1.3522e-01, -5.4212e-01, -2.8370e-01, -3.2960e-01,  1.2828e+00,\n",
       "         -3.6195e-01, -3.8205e-01, -1.9299e-01, -2.4357e-01, -9.0572e-01,\n",
       "          2.2823e-02, -1.8522e-01,  6.1199e-01, -7.0988e-01, -6.9918e-01,\n",
       "         -1.8766e-03, -1.0423e-01, -5.3260e-01, -3.5241e-01,  4.8162e-01,\n",
       "          5.8164e-01, -2.0681e-01,  4.7418e-04,  8.1469e-02,  1.6807e-01,\n",
       "         -4.0084e-01, -6.5389e-01,  1.1395e+00, -4.7898e-01, -1.1043e-01,\n",
       "          5.4090e-01,  3.8815e-01,  7.3758e-03,  3.3981e-01,  3.5074e-01,\n",
       "         -4.0753e-02,  4.4925e-02, -2.3312e-01, -1.2115e+00,  6.5097e-01,\n",
       "          8.5435e-02, -2.3660e-01, -4.2558e-01,  6.2453e-03, -6.2428e-02,\n",
       "         -3.6804e-01, -9.5777e-01, -1.0350e+00, -6.4053e-01, -3.9772e-01,\n",
       "         -1.0635e-03,  1.1103e+00,  5.3058e-01,  2.5864e-01, -3.5753e-01,\n",
       "         -2.3481e-01,  3.8719e-01,  3.1841e-01,  1.7448e-01,  8.0793e-01,\n",
       "          4.9697e-01, -2.5670e-01, -5.6648e-01, -9.0214e-01,  2.9371e-01,\n",
       "          6.7158e-01,  8.9884e-01, -5.2922e-01,  4.4520e-01,  2.4091e-01,\n",
       "         -4.1337e-01, -4.1314e-01,  1.4349e+00,  1.7659e-02, -4.8231e-01,\n",
       "          1.2871e-01, -4.3722e-01,  8.1288e-01, -4.9559e-02, -1.7936e-01,\n",
       "         -1.1891e+00,  2.1217e-01,  4.5148e-01, -2.4368e-03,  2.5503e-01,\n",
       "          4.5936e-02, -4.7914e-01,  2.8133e-01,  8.4497e-02, -4.9861e-02,\n",
       "          8.3689e-01,  3.9308e-01,  1.2698e-01,  1.0310e-01,  5.8426e-01,\n",
       "          2.9657e-01, -9.8044e-01,  1.8323e-01, -7.2138e-03, -1.5243e-02,\n",
       "         -1.2492e+00, -8.8776e-01,  5.7794e-01, -1.3562e+00,  3.2621e-01,\n",
       "         -9.1238e-01, -3.3145e-01, -5.3428e-01, -1.2623e-01, -1.2705e-02,\n",
       "          7.0739e-02,  6.1699e-01, -4.5226e-01,  2.1292e-01,  4.9854e-01,\n",
       "         -4.1111e-01,  3.8267e-01,  2.2687e-02, -2.9113e-01,  8.0511e-02,\n",
       "          4.2199e-02,  8.7279e-01, -8.4425e-01, -1.5370e-01, -2.0582e-01,\n",
       "          2.0470e-01,  4.0536e-01, -6.0140e-01,  7.9615e-01,  7.4778e-01,\n",
       "          4.4083e-01,  1.4455e-01, -4.7461e-01, -5.8060e-01, -7.8567e-01,\n",
       "          7.6434e-02,  2.3062e-01, -4.5125e-01, -1.0561e+00,  1.6969e-01,\n",
       "         -7.4177e-01,  7.1194e-01,  6.3970e-01, -7.5542e-01, -4.7483e-01,\n",
       "         -4.3773e-02, -2.2851e-01, -1.1455e-01,  5.1236e-02, -2.9571e-01,\n",
       "          4.5357e-01, -2.1279e-01, -1.7894e-01, -3.8934e-01,  6.3769e-01,\n",
       "         -2.9407e-01,  7.6349e-02, -4.1954e-01, -5.3324e-01, -4.0661e-01,\n",
       "          3.0226e-01, -5.5102e-01,  2.8145e-01,  1.1392e-01, -1.0438e+00,\n",
       "          2.5877e-01, -9.7528e-02,  1.5607e-01,  8.6711e-02,  1.1426e-01,\n",
       "          1.5055e-01, -1.9005e-01,  3.4826e-01, -5.5750e-01, -3.7245e-01,\n",
       "         -1.0146e-01,  8.0674e-01,  4.0843e-01,  8.1869e-01,  1.2444e-03,\n",
       "          6.2514e-01, -8.9562e-01, -7.8104e-01,  1.8749e-01,  2.3599e-01,\n",
       "         -9.8488e-01,  1.6433e-01,  2.0267e-01, -7.8865e-01, -5.5077e-01,\n",
       "         -3.7620e-01, -8.1326e-02,  7.5854e-02, -5.1127e-01,  5.4202e-04,\n",
       "          3.2047e-01,  2.7479e-01, -1.2046e-01, -2.4667e-02, -3.6420e-03,\n",
       "          1.5106e+00,  4.9681e-01, -2.5335e-01, -7.6929e-01,  6.9212e-01,\n",
       "         -5.0369e-01, -3.0040e-01,  6.8267e-01,  3.3815e-01,  6.0848e-01,\n",
       "          3.8677e-01,  3.5713e-01, -6.2177e-01, -2.7171e-01,  2.1406e-01,\n",
       "          7.2708e-01, -9.5020e-01, -2.2848e-01, -2.2006e-01,  1.6580e-02,\n",
       "          1.9636e-01, -6.0296e-01, -7.1853e-01,  2.2888e-01,  8.6585e-01,\n",
       "          5.5921e-01, -2.9839e-01, -5.7910e-01,  1.0117e-01, -1.0617e-01,\n",
       "         -7.9934e-02, -1.4270e-01,  1.0770e+00,  7.2587e-01,  1.6324e-01,\n",
       "          8.0698e-01,  1.5827e-01, -3.2184e-01,  2.6292e-01,  7.8670e-01,\n",
       "          1.0593e-01, -4.1134e-01,  9.2593e-02, -4.8063e-01, -9.3222e-01,\n",
       "          3.9530e-01,  6.3784e-01,  9.7574e-01, -4.4546e-01, -8.5300e-01,\n",
       "         -6.0900e-01,  1.8074e-01, -4.5369e-01, -6.3513e-01, -9.8931e-02,\n",
       "         -2.6329e-01, -4.1342e-03,  7.3018e-01,  6.0828e-01,  5.1781e-01,\n",
       "          8.0718e-01,  9.2596e-01,  5.5483e-01,  4.5343e-01, -1.8449e-01,\n",
       "          1.1333e+00,  2.4994e-02,  4.7305e-02, -5.4664e-02, -5.9540e-01,\n",
       "         -7.5238e-01, -1.1010e+00, -5.5088e-01,  3.6862e-02,  7.5868e-01,\n",
       "         -1.3246e-01,  9.3265e-02,  3.6642e-01,  1.2307e-01, -7.1625e-01,\n",
       "         -4.8316e-01, -6.5308e-02, -2.6259e-01, -8.9303e-02,  2.2329e-01,\n",
       "         -4.5838e-01, -4.3622e-01, -5.2564e-01, -6.9890e-01, -4.8984e-01,\n",
       "         -9.9606e-01,  3.1710e-01, -9.1616e-01, -2.3647e-01,  7.4194e-01,\n",
       "         -2.4972e-01, -9.7356e-02,  3.6878e-01,  5.2523e-02,  2.2013e-01,\n",
       "         -6.0277e-01, -1.5183e-01,  6.5993e-01,  4.7337e-02, -1.5968e-01,\n",
       "          3.1903e-01, -1.9716e-02, -7.8839e-02, -6.7438e-01, -1.0272e+00,\n",
       "         -1.7476e-01, -3.8599e-01, -2.2454e-01, -2.5076e-01, -1.3255e-01,\n",
       "         -1.1147e-01,  3.9397e-01, -2.8769e-02, -4.4064e-02,  7.3007e-01,\n",
       "         -4.7073e-01,  2.1270e-01, -3.2566e-01, -3.8015e-01,  1.2481e-01,\n",
       "          5.0382e-01,  4.3539e-01,  9.0453e-01, -3.1581e-01,  1.9015e-01,\n",
       "         -2.4156e-01, -2.5505e-01, -7.5678e-01, -5.0481e-01,  4.6987e-01,\n",
       "          3.8720e-01, -2.5167e-01,  8.2457e-01, -2.4133e-01,  4.0972e-01,\n",
       "          1.1456e-02, -1.2878e+00, -1.3442e+00,  1.7654e-01,  3.4311e-01,\n",
       "         -4.3115e-01, -3.6243e-01,  8.9564e-01,  5.2909e-02, -8.0105e-01,\n",
       "         -8.2314e-01, -8.6927e-01,  2.7447e-01,  7.0583e-02,  3.3634e-01,\n",
       "          7.7659e-01, -6.0782e-01,  2.9964e-01, -6.6231e-02,  1.7491e-01,\n",
       "         -9.5040e-01, -6.7377e-01, -3.9076e-01, -8.6991e-01, -1.5314e-01,\n",
       "          2.5975e-01, -7.7465e-01,  1.3933e-01,  5.9862e-01, -4.5489e-01,\n",
       "          1.9746e-01,  3.3275e-01, -2.9929e-01, -1.8485e-02, -4.0825e-01,\n",
       "         -9.4730e-01,  3.5926e-01, -3.5499e-01, -1.3086e-01,  5.3178e-01,\n",
       "          4.3386e-01, -3.6170e-01,  1.9026e-01,  9.0393e-02,  7.3275e-02,\n",
       "          3.6356e-01, -7.1849e-01, -2.8638e-01,  4.1428e-01, -3.9606e-01,\n",
       "          2.7293e-01,  5.9827e-01,  6.0027e-02, -7.2992e-01,  8.4431e-01,\n",
       "          4.4223e-02, -2.4789e-01,  9.1568e-01,  4.7646e-02,  1.2744e-01,\n",
       "         -2.2415e-01,  6.9608e-01,  7.7978e-01,  6.6028e-01,  4.5900e-01,\n",
       "         -2.9181e-01,  1.5351e-01, -1.1193e-01,  6.5780e-01, -8.9942e-02,\n",
       "          1.6117e-01, -2.1468e-01,  7.8392e-01,  1.3984e-01,  4.8653e-01,\n",
       "          6.7024e-01, -3.7494e-01, -9.0121e-01,  2.8365e-01,  1.1493e-01,\n",
       "         -7.1606e-01, -3.9976e-01,  2.6317e-01,  5.3477e-01, -1.4743e-01,\n",
       "          2.5073e-01,  7.5130e-02,  2.4564e-01, -2.8243e-01, -4.7125e-01,\n",
       "         -1.3405e-01, -1.0020e+00,  1.2049e-01,  7.1229e-01, -2.9538e-01,\n",
       "          4.8168e-01, -4.1493e-01, -3.9869e-02, -3.2978e-01,  2.8404e-02,\n",
       "          1.1828e+00, -1.3820e+00, -6.9452e-02, -4.4678e-01,  1.5731e-02,\n",
       "          2.6181e-01, -3.2157e-01,  9.6155e-01,  3.7801e-01, -1.9269e-01,\n",
       "         -1.7314e-01,  7.3592e-02,  4.4219e-01, -1.2617e-01, -5.1110e-02,\n",
       "          9.3943e-01, -4.6563e-01, -6.0675e-02, -7.6279e-01,  8.4514e-01,\n",
       "          7.7486e-01, -5.0467e-02, -5.0054e-01,  3.7878e-02,  5.2309e-01,\n",
       "         -2.1711e-01,  2.0385e-01, -3.3318e-01, -6.3699e-01,  3.3129e-01,\n",
       "         -2.6388e-01, -2.6700e-03,  6.0101e-01,  1.2737e-01, -9.2485e-01,\n",
       "         -2.6031e-01, -1.3434e+00, -5.0736e-01,  2.0363e-02, -2.5627e-01,\n",
       "         -8.3358e-02, -1.2216e-01,  1.0572e+00,  2.3956e-01, -5.2753e-01,\n",
       "         -1.5223e-02, -6.2680e-01, -8.9458e-01,  1.2524e+00, -3.2484e-01,\n",
       "          3.6237e-01,  9.3434e-02, -1.0472e+00, -3.9509e-01,  7.7104e-01,\n",
       "          3.2011e-01,  4.0059e-01, -4.8364e-01, -1.5088e-01, -7.4627e-02,\n",
       "         -3.0479e-01,  6.7093e-01,  2.2957e-01, -2.0791e-01,  5.2717e-01,\n",
       "          9.3323e-01, -1.0825e+00, -3.1174e-01, -1.5827e-02,  1.2442e+00,\n",
       "          5.5488e-01,  6.2027e-01, -1.0681e-01, -3.9327e-01, -2.1300e-01,\n",
       "          5.5424e-01, -7.2750e-02,  3.1279e-01, -2.7100e-01, -1.4312e-01,\n",
       "          2.2679e-01, -6.0588e-01, -1.9381e-01, -6.9304e-01,  6.1587e-01,\n",
       "         -5.3393e-01,  5.8797e-01,  1.1638e+00,  7.9308e-01, -4.0408e-01,\n",
       "          1.2948e+00,  1.4374e+00,  2.0126e-01, -1.1405e+00, -4.5900e-02,\n",
       "         -3.6998e-01,  5.0943e-01, -2.1623e-01, -2.6573e-01,  7.7431e-01,\n",
       "          2.2721e-01,  5.6474e-02, -4.4416e-02,  5.2635e-03, -1.0685e+00,\n",
       "          3.6883e-01,  8.8217e-02,  2.2068e-01, -2.0530e-01, -9.7139e-01,\n",
       "         -7.2784e-01,  4.9624e-01, -2.5262e-01,  3.8770e-01,  1.2489e-01,\n",
       "          1.9817e-01,  1.7978e-01,  7.4349e-01, -5.2281e-01,  1.0590e-01,\n",
       "          8.0208e-01, -1.4051e+00, -3.5675e-01, -6.9716e-01,  4.9553e-01,\n",
       "          3.7993e-01,  9.6715e-01, -2.7552e-01, -1.7793e-01, -1.6232e-01,\n",
       "         -2.0458e-01, -1.0814e-01, -2.7346e-01,  3.3321e-01,  2.2827e-02,\n",
       "          3.0918e-01,  4.1660e-01, -6.9011e-02, -1.3617e+00,  7.3524e-02,\n",
       "          4.5932e-01, -1.2993e+00,  9.7480e-02, -5.6932e-01, -8.9301e-02,\n",
       "          3.3482e-02, -9.3444e-02, -5.9551e-01,  7.7543e-02,  1.2637e+00,\n",
       "          1.2270e-01, -8.8204e-01,  4.4031e-01,  6.6829e-01,  3.7268e-01,\n",
       "         -1.1770e-01,  8.9749e-01,  2.9996e-01,  2.3941e-01, -6.3169e-02,\n",
       "         -7.3643e-01, -2.6336e-01, -3.9784e-01,  8.4590e-02, -1.0757e+00,\n",
       "         -2.6036e-01, -2.7678e-01, -2.0528e-01, -1.7229e-01,  6.9267e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model(torch.rand(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b8d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simsim",
   "language": "python",
   "name": "simsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
